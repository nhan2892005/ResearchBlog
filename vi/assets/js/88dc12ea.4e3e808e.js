"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[963],{756:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>m});var i=r(4848),t=r(8453);const o={},a="Dynamic Programming",c={id:"ReinforcementLearning/DynamicProgramming/index",title:"Dynamic Programming",description:"",source:"@site/docs/ReinforcementLearning/DynamicProgramming/index.md",sourceDirName:"ReinforcementLearning/DynamicProgramming",slug:"/ReinforcementLearning/DynamicProgramming/",permalink:"/ResearchBlog/vi/docs/ReinforcementLearning/DynamicProgramming/",draft:!1,unlisted:!1,editUrl:"https://github.com/nhan2892005/ResearchBlog/docs/ReinforcementLearning/DynamicProgramming/index.md",tags:[],version:"current",frontMatter:{},sidebar:"RLsidebar",previous:{title:"Markov Decision Processes",permalink:"/ResearchBlog/vi/docs/ReinforcementLearning/MarkovDecisionProcesses/"},next:{title:"Monte Carlo Methods",permalink:"/ResearchBlog/vi/docs/ReinforcementLearning/MonteCarlos/"}},s={},m=[];function g(e){const n={h1:"h1",header:"header",...(0,t.R)(),...e.components};return(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"dynamic-programming",children:"Dynamic Programming"})})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>c});var i=r(6540);const t={},o=i.createContext(t);function a(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);