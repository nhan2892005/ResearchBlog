"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[963],{756:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>m});var t=r(4848),i=r(8453);const o={},a="Dynamic Programming",c={id:"ReinforcementLearning/DynamicProgramming/index",title:"Dynamic Programming",description:"",source:"@site/docs/ReinforcementLearning/DynamicProgramming/index.md",sourceDirName:"ReinforcementLearning/DynamicProgramming",slug:"/ReinforcementLearning/DynamicProgramming/",permalink:"/ResearchBlog/docs/ReinforcementLearning/DynamicProgramming/",draft:!1,unlisted:!1,editUrl:"https://github.com/nhan2892005/ResearchBlog/docs/ReinforcementLearning/DynamicProgramming/index.md",tags:[],version:"current",frontMatter:{},sidebar:"RLsidebar",previous:{title:"Markov Decision Processes",permalink:"/ResearchBlog/docs/ReinforcementLearning/MarkovDecisionProcesses/"},next:{title:"Monte Carlo Methods",permalink:"/ResearchBlog/docs/ReinforcementLearning/MonteCarlos/"}},s={},m=[];function g(e){const n={h1:"h1",header:"header",...(0,i.R)(),...e.components};return(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"dynamic-programming",children:"Dynamic Programming"})})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(g,{...e})}):g(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>c});var t=r(6540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);