"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[559],{9204:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>f});var i=t(4848),o=t(8453);const r={},c=void 0,s={id:"ReinforcementLearning/OffOnPolicy/index",title:"index",description:"",source:"@site/docs/ReinforcementLearning/OffOnPolicy/index.md",sourceDirName:"ReinforcementLearning/OffOnPolicy",slug:"/ReinforcementLearning/OffOnPolicy/",permalink:"/ResearchBlog/vn/docs/ReinforcementLearning/OffOnPolicy/",draft:!1,unlisted:!1,editUrl:"https://github.com/nhan2892005/ResearchBlog/docs/ReinforcementLearning/OffOnPolicy/index.md",tags:[],version:"current",frontMatter:{},sidebar:"RLsidebar",previous:{title:"Deep RL",permalink:"/ResearchBlog/vn/docs/ReinforcementLearning/DeepRL/"},next:{title:"Application of Reinforcement Learning",permalink:"/ResearchBlog/vn/docs/ReinforcementLearning/Applications/"}},a={},f=[];function l(e){return(0,i.jsx)(i.Fragment,{})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l()}},8453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function c(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);