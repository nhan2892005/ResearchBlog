"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[963],{756:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>m,contentTitle:()=>a,default:()=>g,frontMatter:()=>o,metadata:()=>c,toc:()=>s});var t=r(4848),i=r(8453);const o={},a="Dynamic Programming",c={id:"ReinforcementLearning/DynamicProgramming/index",title:"Dynamic Programming",description:"",source:"@site/docs/ReinforcementLearning/DynamicProgramming/index.md",sourceDirName:"ReinforcementLearning/DynamicProgramming",slug:"/ReinforcementLearning/DynamicProgramming/",permalink:"/vn/docs/ReinforcementLearning/DynamicProgramming/",draft:!1,unlisted:!1,editUrl:"https://github.com/nhan2892005/ResearchBlog/docs/ReinforcementLearning/DynamicProgramming/index.md",tags:[],version:"current",frontMatter:{},sidebar:"RLsidebar",previous:{title:"Markov Decision Processes",permalink:"/vn/docs/ReinforcementLearning/MarkovDecisionProcesses/"},next:{title:"Monte Carlo Methods",permalink:"/vn/docs/ReinforcementLearning/MonteCarlos/"}},m={},s=[];function d(n){const e={h1:"h1",header:"header",...(0,i.R)(),...n.components};return(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"dynamic-programming",children:"Dynamic Programming"})})}function g(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>c});var t=r(6540);const i={},o=t.createContext(i);function a(n){const e=t.useContext(o);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);