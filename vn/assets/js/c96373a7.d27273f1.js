"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[656],{8084:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var r=t(4848),o=t(8453);const i={},c="Reinforcement Learning",s={id:"ReinforcementLearning/Introduction/intro",title:"Reinforcement Learning",description:"",source:"@site/docs/ReinforcementLearning/Introduction/intro.md",sourceDirName:"ReinforcementLearning/Introduction",slug:"/ReinforcementLearning/Introduction/intro",permalink:"/ResearchBlog/vn/docs/ReinforcementLearning/Introduction/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/nhan2892005/ResearchBlog/docs/ReinforcementLearning/Introduction/intro.md",tags:[],version:"current",frontMatter:{},sidebar:"RLsidebar",next:{title:"Markov Decision Processes",permalink:"/ResearchBlog/vn/docs/ReinforcementLearning/MarkovDecisionProcesses/"}},a={},d=[];function u(e){const n={h1:"h1",header:"header",...(0,o.R)(),...e.components};return(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"reinforcement-learning",children:"Reinforcement Learning"})})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>s});var r=t(6540);const o={},i=r.createContext(o);function c(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);